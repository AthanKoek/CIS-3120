{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1cd5414-8dcd-46e7-89c0-ed6c576e2081",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip3 install pandas\n",
    "%reset\n",
    "import time\n",
    "import json\n",
    "import requests as r\n",
    "import pandas as pd\n",
    "from io import StringIO\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.dates as mdates\n",
    "\n",
    "from config import API_KEY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e987cff5-1456-4221-8db7-5e284e26e051",
   "metadata": {},
   "outputs": [],
   "source": [
    "#1A and 1B.\n",
    "\n",
    "#Establish parameters to be used\n",
    "#query = '' # scans through articles for this keyword\n",
    "fq = 'section_name:(\"Health\")' # Filters for articles under this section\n",
    "begin_date = '20180101'\n",
    "end_date = '20231231'\n",
    "\n",
    "# Note - NYtimes API has a pagination limit of 10 results per page - so loop through pages and combine information into dataframe\n",
    "# Note - NYtimes API has a call limit of 500 requests per day and 5 requests per minute - so recommended to use sleep 12 seconds to avoid hitting the minute rate limit\n",
    "\n",
    "# Create an empty datatframe \n",
    "articles_df = pd.DataFrame()\n",
    "\n",
    "\n",
    "\n",
    "# Iterate through 0-X pages(5 in this case) with each page containg 10 articles (Index 0 -9)\n",
    "for pages in range(5):\n",
    "    url = f\"https://api.nytimes.com/svc/search/v2/articlesearch.json?fq={fq}&page={pages}&begin_date={begin_date}&end_date={end_date}&api-key={API_KEY}\"\n",
    "    response = r.get(url).content.decode()\n",
    "    response_json = json.loads(response)\n",
    "\n",
    "    pages_df = pd.json_normalize(response_json['response']['docs'])\n",
    "    articles_df = pd.concat([articles_df, pages_df], ignore_index=True)\n",
    "\n",
    "    # Note: For any range higher than 5 it is recommended to use time.sleep(12) to avoid the call limit rate\n",
    "    #time.sleep(12) # Causes the cell to take a long moment before generating output\n",
    "\n",
    "\n",
    "    # TO error check when dataframe is not being generated\n",
    "    #if 'response' in response_json and 'docs' in response_json['response']:\n",
    "        #pages_df = pd.json_normalize(response_json['response']['docs'])\n",
    "        #articles_df = pd.concat([articles_df, pages_df], ignore_index=True)\n",
    "        #time.sleep(12)\n",
    "    #else:\n",
    "        #print(f\"Page {pages} did not return the expected data.\")\n",
    "\n",
    "articles_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "965dd0b4-37bb-494c-bcaa-69d678f8bd86",
   "metadata": {},
   "outputs": [],
   "source": [
    "#2A.\n",
    "#keywords_series = articles_df['keywords'].explode()\n",
    "#keywords_count = keywords_series.value_counts()\n",
    "#keywords_count = keywords_count.reset_index()\n",
    "\n",
    "#keywords_count\n",
    "# keywords column includes various information that I don't want"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d203619-cd71-40ad-bdd9-6070b201c88f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#2A.\n",
    "\n",
    "#Create a loop that will take the value of the associated value key and eventually add it back to dataframe in a new column\n",
    "\n",
    "# Empty list to hold values of all rows\n",
    "keyword_values = []\n",
    "\n",
    "# Iterate/loop over each row in the 'keywords' column\n",
    "for keywords_list in articles_df['keywords']:\n",
    "    # Initialize an empty list to hold the keyword values for the iterated row \n",
    "    row_values = []\n",
    "    # Iterate/loop over each dictionary in the current list of keywords\n",
    "    for keyword_dict in keywords_list:  \n",
    "        if 'value' in keyword_dict:\n",
    "            # Add the value associated with the 'value' key to the row_values list\n",
    "            row_values.append(keyword_dict['value'])\n",
    "    # Adds the list of keyword values for this row to the keyword_values list\n",
    "    keyword_values.append(row_values)\n",
    "\n",
    "# Assign the list of keyword values back to the DataFrame in a new column\n",
    "articles_df['keyword_values'] = keyword_values\n",
    "\n",
    "# exploded 'keyword_values' column creates a row for each keyword string\n",
    "exploded_keywords = articles_df['keyword_values'].explode()\n",
    "\n",
    "# Count occurrences of each keyword\n",
    "keywords_count = exploded_keywords.value_counts()\n",
    "\n",
    "keywords_count = keywords_count.reset_index()\n",
    "\n",
    "\n",
    "print(keywords_count.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c37f114c-7712-46d3-b304-8dafee62bef3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#2B.\n",
    "\n",
    "articles_df['pub_date'] = pd.to_datetime(articles_df['pub_date']) # This line is needed since yo uneed datetimeline values for .dt\n",
    "\n",
    "# Extract the year from 'pub_date' and put in new column\n",
    "articles_df['year'] = articles_df['pub_date'].dt.year\n",
    "\n",
    "# Creates a DataFrame that includes both the year and the exploded keyword values\n",
    "keywords_by_year_df = articles_df.explode('keyword_values')[['year', 'keyword_values']]\n",
    "\n",
    "# Group by both 'year' and 'keyword_values'\n",
    "#.size()  returns a pandas series that possess the total number of row count for each group\n",
    "keywords_frequency_over_time = keywords_by_year_df.groupby(['year', 'keyword_values']).size().reset_index(name='count')\n",
    "\n",
    "# Sort the DataFrame by keyword and year\n",
    "keywords_frequency_over_time = keywords_frequency_over_time.sort_values(by=['keyword_values', 'year'])\n",
    "\n",
    "keywords_frequency_over_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6d63a5b-37bd-445e-891a-de586007d44e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#2B.\n",
    "# Filters dataframe to look through keywrod_values and their count over time \n",
    "user_input = input(\"Which keyword would you like to analyze\")\n",
    "keyword_trend = keywords_frequency_over_time[keywords_frequency_over_time['keyword_values'] == user_input]\n",
    "\n",
    "keyword_trend\n",
    "\n",
    "#For example: Coronavirus (2019-nCoV) had a count of 17 counts in 2021 when the pandemic may have been at its peak, but it's gone down since\n",
    "#Note: Only analyzes from the first 10 pages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5fe3277-dbca-4a57-a7db-7052cb127f51",
   "metadata": {},
   "outputs": [],
   "source": [
    "#3A.\n",
    "\n",
    "# Bar Chart\n",
    "sns.set_style(\"whitegrid\")\n",
    "\n",
    "# Set variable as top 10 keywords for data in graph\n",
    "top_keywords = keywords_count.head(10)\n",
    "\n",
    "plt.figure(figsize=(10, 6)) # Width of 10 inch and 6 inch\n",
    "#plt.xticks(range(min(top_keywords['count']),max(top_keywords['count'])))\n",
    "sns.barplot(x='count', y='keyword_values', data = top_keywords, errorbar = None)\n",
    "#\n",
    "\n",
    "plt.title('Keyword Frequencies')\n",
    "plt.xlabel('Frequency')\n",
    "plt.ylabel('Keywords')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf2a1301-a8ce-46b7-bbd9-d5bea657059c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#3B \n",
    "# make sure 'pub_date' is in datetime format\n",
    "articles_df['pub_date'] = pd.to_datetime(articles_df['pub_date'])\n",
    "\n",
    "# Filter the DataFrame for rows where the year is 2018\n",
    "articles_2018 = articles_df[articles_df['pub_date'].dt.year == 2018]\n",
    "\n",
    "# Resample to weekly counts, using the start of each week\n",
    "# https://stackoverflow.com/questions/14530556/resample-time-series-in-pandas-to-a-weekly-interval\n",
    "# https://towardsdatascience.com/resample-function-of-pandas-79b17ec82a78\n",
    "articles_per_week_2018 = articles_2018.resample('W', on='pub_date').size().reset_index(name='count')\n",
    "\n",
    "# Plotting the weekly trend for 2018\n",
    "plt.figure(figsize=(14, 6)) \n",
    "sns.lineplot(data=articles_per_week_2018, x='pub_date', y='count', marker='o')\n",
    "plt.xlabel('Week')\n",
    "plt.ylabel('Number of Articles')\n",
    "plt.title('Weekly Number of Articles Published in 2018')\n",
    "\n",
    "# Formats the x-axis ticks to the start of each month and to display the month and year, \n",
    "plt.gca().xaxis.set_major_locator(mdates.MonthLocator())\n",
    "plt.gca().xaxis.set_major_formatter(mdates.DateFormatter('%b %Y'))\n",
    "\n",
    "plt.xticks(rotation=45) # Rotating dates for better readability\n",
    "\n",
    "plt.tight_layout() # Adjust layout to make room for the rotated date labels\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b38bc124-6238-4955-8bf2-507a927b919b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#3\n",
    "# The plot is overly crowded when put together, so use loop that will generate a plot for each year\n",
    "for year in range(2018, 2024):\n",
    "    \n",
    "    articles_year = articles_df[articles_df['pub_date'].dt.year == year]\n",
    "    \n",
    "    #\n",
    "    articles_per_week = articles_year.resample('W', on='pub_date').size().reset_index(name='count')\n",
    "    \n",
    "    sns.set_style(\"whitegrid\")\n",
    "    plt.figure(figsize=(14, 6))  \n",
    "    sns.lineplot(data=articles_per_week, x='pub_date', y='count', marker='o')\n",
    "    plt.xlabel('Week')\n",
    "    plt.ylabel('Number of Articles')\n",
    "    plt.title(f'Weekly Number of Articles Published in {year}')\n",
    "    \n",
    "    \n",
    "    plt.gca().xaxis.set_major_locator(mdates.MonthLocator())\n",
    "    plt.gca().xaxis.set_major_formatter(mdates.DateFormatter('%b %Y'))\n",
    "    \n",
    "    plt.xticks(rotation=45) # Rotating dates for better readability\n",
    "    \n",
    "    plt.tight_layout() # Adjust layout to make room for the rotated date labels\n",
    "    \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05fc9447-632d-46f0-87e5-4db0382b4233",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
